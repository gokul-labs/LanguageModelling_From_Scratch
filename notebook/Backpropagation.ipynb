{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "2afc579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "names =[]\n",
    "with open(\"../code/names.txt\") as file:\n",
    "    names = file.read().split(\"\\n\")\n",
    "    \n",
    "stoi = {c:i for i, c in enumerate(string.ascii_lowercase)}\n",
    "itos = {c:i for i, c in stoi.items()}\n",
    "\n",
    "stoi[\".\"] = 26\n",
    "itos[26] = \".\"\n",
    "\n",
    "BLOCK_SIZE = 3\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    context = [26] * BLOCK_SIZE\n",
    "    for c in name + \".\":\n",
    "        label = stoi[c]\n",
    "        inputs.append(context)\n",
    "        labels.append(label)\n",
    "#         print(\"\".join(itos[i] for i in context), '-->', itos[label])\n",
    "        context = context[1:] + [label]\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b789af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 10\n",
    "VOCABULARY_SIZE = 27 # 26 alphabets and one special start end char -> \".\"\n",
    "HIDDEN_LAYER_NEURONS = 100\n",
    "\n",
    "CHAR_EMBEDDINGS = torch.randn((VOCABULARY_SIZE, EMBEDDING_DIMENSION)) \n",
    "WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs]\n",
    "\n",
    "W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS)) * ((5/3) / (EMBEDDING_DIMENSION * BLOCK_SIZE)**0.5)\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS) * 0.1\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE)) * ((5/3) / HIDDEN_LAYER_NEURONS**0.5)\n",
    "b2 = torch.randn(VOCABULARY_SIZE) * 0.1\n",
    "\n",
    "# Mul by 0.1 as 1s and 0s mask out smaller changes in grad computation. But we just \n",
    "# want to see those as well during this experiment\n",
    "\n",
    "bngains = torch.randn((1, HIDDEN_LAYER_NEURONS)) * 0.1 + 1.0\n",
    "bnoffsets = torch.randn((1, HIDDEN_LAYER_NEURONS)) * 0.1\n",
    "\n",
    "bnmean_running = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "bnstd_running = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2, bngains, bnoffsets]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b87483f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9552, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPSILON = 1e-5\n",
    "# Batching    \n",
    "batch_idx = torch.randint(0, inputs.shape[0], (BATCH_SIZE,))\n",
    "\n",
    "# Prepping input to the network\n",
    "WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs[batch_idx]]\n",
    "WORD_EMBEDDINGS_CAT = WORD_EMBEDDINGS.view(WORD_EMBEDDINGS.shape[0], -1)\n",
    "batch_labels = labels[batch_idx]\n",
    "\n",
    "# Layer 1 \n",
    "pre_batchnorm = WORD_EMBEDDINGS_CAT @ W1 + b1\n",
    "# Batch Norm\n",
    "# Mean = Sum / no. of elements; Variance\n",
    "bnmeani = pre_batchnorm.sum(0, keepdim=True) * (1/BATCH_SIZE)\n",
    "bndiff = pre_batchnorm - bnmeani\n",
    "bndiff_squared = bndiff**2\n",
    "bnvar = bndiff_squared.sum(0, keepdim=True) * (1/(BATCH_SIZE-1)) # Bessel's correction\n",
    "bnvar_inverse = (bnvar + EPSILON)**-0.5\n",
    "# Normalize \n",
    "bnraw = bndiff * bnvar_inverse\n",
    "# Preactivation\n",
    "pre_activation_1 = bngains * bnraw + bnoffsets \n",
    "# Activation\n",
    "h = torch.tanh(pre_activation_1)\n",
    "\n",
    "# Layer 2\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# Cross entropy\n",
    "logit_maxes = logits.max(1, keepdim=True).values # Find max of each output tensor\n",
    "norm_logits = logits - logit_maxes # Subtract corresp. max for math convenience\n",
    "counts = norm_logits.exp() # Exponentiate\n",
    "counts_sum = counts.sum(1, keepdim=True) # Count sum of corres. output tensor to normalize \n",
    "                                         # and prepare to convert counts to prob\n",
    "counts_sum_inverse = counts_sum**-1\n",
    "probs = counts * counts_sum_inverse # Convert to probabilities\n",
    "logprobs = probs.log() # Take log\n",
    "loss = -logprobs[range(BATCH_SIZE), batch_labels].mean() # Compute NLL Loss for current batch\n",
    "\n",
    "# Zero grad\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "# Backward Pass\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inverse, \n",
    "          norm_logits, logit_maxes, logits, h, pre_activation_1, bnraw,\n",
    "         bnvar_inverse, bnvar, bndiff_squared, bndiff, pre_batchnorm, bnmeani,\n",
    "         bnstdi, WORD_EMBEDDINGS, WORD_EMBEDDINGS_CAT, CHAR_EMBEDDINGS]:\n",
    "    t.retain_grad()\n",
    "loss.backward() \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "84ff5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare manual grad vs pytorch grad\n",
    "def cmp(measure_name, derivative_computed, actual_variable):\n",
    "    exact = torch.all(derivative_computed == actual_variable.grad).item()\n",
    "    approx = torch.allclose(derivative_computed, actual_variable.grad)\n",
    "    max_diff = (derivative_computed - actual_variable.grad).abs().max().item()\n",
    "    print(f'{measure_name:20} | exact: {str(exact):4s} | approximate: {str(approx):4s} | maxdiff: {max_diff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "82c86b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs             | exact: True | approximate: True | maxdiff: 0.0\n",
      "probs                | exact: True | approximate: True | maxdiff: 0.0\n",
      "counts_sum_inverse   | exact: True | approximate: True | maxdiff: 0.0\n",
      "counts               | exact: False | approximate: False | maxdiff: 0.012354373931884766\n",
      "counts_sum           | exact: True | approximate: True | maxdiff: 0.0\n",
      "counts               | exact: True | approximate: True | maxdiff: 0.0\n",
      "norm_logits          | exact: True | approximate: True | maxdiff: 0.0\n",
      "logits               | exact: False | approximate: True | maxdiff: 6.51925802230835e-09\n",
      "logits               | exact: True | approximate: True | maxdiff: 0.0\n",
      "h                    | exact: True | approximate: True | maxdiff: 0.0\n",
      "b2                   | exact: True | approximate: True | maxdiff: 0.0\n",
      "W2                   | exact: True | approximate: True | maxdiff: 0.0\n",
      "pre_activation_1     | exact: True | approximate: True | maxdiff: 0.0\n",
      "bngains              | exact: True | approximate: True | maxdiff: 0.0\n",
      "bnraw                | exact: True | approximate: True | maxdiff: 0.0\n",
      "bnoffsets            | exact: True | approximate: True | maxdiff: 0.0\n",
      "bndiff               | exact: False | approximate: False | maxdiff: 0.0021139285527169704\n",
      "dbnvar_inverse       | exact: True | approximate: True | maxdiff: 0.0\n",
      "bnvar                | exact: True | approximate: True | maxdiff: 0.0\n",
      "bndiff_squared       | exact: True | approximate: True | maxdiff: 0.0\n",
      "bndiff               | exact: True | approximate: True | maxdiff: 0.0\n",
      "pre_batchnorm        | exact: False | approximate: False | maxdiff: 0.001818929798901081\n",
      "bnmeani              | exact: True | approximate: True | maxdiff: 0.0\n",
      "pre_batchnorm        | exact: True | approximate: True | maxdiff: 0.0\n",
      "WORD_EMBEDDINGS_CAT  | exact: True | approximate: True | maxdiff: 0.0\n",
      "W1                   | exact: True | approximate: True | maxdiff: 0.0\n",
      "b1                   | exact: True | approximate: True | maxdiff: 0.0\n",
      "CHAR_EMBEDDINGS      | exact: True | approximate: True | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Manually compute backpropagation through all variables listed above\n",
    "\n",
    "# loss = - (a+b+c)/3 = -a/3 + -b/3 + -c/3\n",
    "# Dl/da = -1/3 ///y for other vars. More generally, -1/n\n",
    "# Other components do not contribute to loss (only pluck out the prob of the correct next\n",
    "# character to compute NLL Loss), hence their grad is zero\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(BATCH_SIZE), batch_labels] = -1/BATCH_SIZE\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "\n",
    "# d/dx(log x) = 1/x\n",
    "dprobs = 1/probs * dlogprobs\n",
    "cmp(\"probs\", dprobs, probs)\n",
    "\n",
    "# probs = counts * counts_sum_inverse . Tensor broadcast\n",
    "# c = a * b\n",
    "# a[3x3] b[3x1]\n",
    "# a[00] * b[0] a[01] * b[0] a[02] * b[0]\n",
    "# a[10] * b[1] a[11] * b[1] a[12] * b[1]\n",
    "# a[20] * b[2] a[21] * b[2] a[22] * b[2]\n",
    "\n",
    "# dc/db = a[3x3]\n",
    "# dcounts_sum_inverse = (counts * dprobs) --> for replicated b, but we have non-replicated\n",
    "# remember from micrograd, if a node is reused, we used += , here we sum \n",
    "dcounts_sum_inverse = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp(\"counts_sum_inverse\", dcounts_sum_inverse, counts_sum_inverse)\n",
    "\n",
    "# dc/da = b\n",
    "dcounts = (counts_sum_inverse * dprobs)\n",
    "cmp(\"counts\", dcounts, counts)\n",
    "# counts          | exact: False | approximate: False | maxdiff: 0.020472588017582893\n",
    "# This is only partial as counts also depends on counts_sum = counts.sum(1, keepdim=True)\n",
    "\n",
    "# m^n = n * m^(n-1)\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inverse\n",
    "cmp(\"counts_sum\", dcounts_sum, counts_sum)\n",
    "\n",
    "# a[3x3] ==> b[3x1]\n",
    "# a[00] + a[01] + a[02] => b[0]\n",
    "# a[10] + a[11] + a[12] => b[1]\n",
    "# a[20] + a[21] + a[22] => b[2]\n",
    "# Addition just backpropogates same value as local gradient is 1.0\n",
    "dcounts += 1.0 * dcounts_sum\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "cmp(\"counts\", dcounts, counts)\n",
    "# Now its true as we += and add on the grad backprop effect from before\n",
    "# counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
    "\n",
    "# e^x = e^x\n",
    "dnorm_logits = counts * dcounts\n",
    "cmp(\"norm_logits\", dnorm_logits, norm_logits)\n",
    "\n",
    "# c[3x3] = a[3x3] - b[3x1]\n",
    "# c[00]  c[01]  c[02] = a[00] - b[0]  a[01] - b[0]  a[02] - b[0]\n",
    "# c[10]  c[11]  c[12] = a[10] - b[1]  a[11] - b[1]  a[12] - b[1]\n",
    "# c[20]  c[21]  c[22] = a[20] - b[2]  a[21] - b[2]  a[22] - b[2]\n",
    "dlogits = dnorm_logits.clone()\n",
    "cmp(\"logits\", dlogits, logits)\n",
    "\n",
    "# Is done for numerical stability and does not really affect the loss. \n",
    "# Hence gradient should be 0. Check, it will be near-zero. \n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "# dlogits used once before. So += .logits.max(1) gives both max vals and indices of max vals\n",
    "# So in one row, except for max's index (to be 1), all else should be 0. One-hot encode\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "cmp(\"logits\", dlogits, logits)\n",
    "\n",
    "# y = mx + c\n",
    "# dy/dm = x\n",
    "# Take 2x2 matrix and easily derivable. Or trick is to make a possible matrix mul with dims\n",
    "dh = dlogits @ W2.T\n",
    "cmp(\"h\", dh, h)\n",
    "db2 = dlogits.sum(0)\n",
    "cmp(\"b2\", db2, b2)\n",
    "dW2 = h.T @ dlogits\n",
    "cmp(\"W2\", dW2, W2)\n",
    "\n",
    "# dtanh x / dx = 1 - t^2 , where t = tanh(x)\n",
    "dpre_activation_1 = (1 - h**2) * dh\n",
    "cmp(\"pre_activation_1\", dpre_activation_1, pre_activation_1)\n",
    "\n",
    "# Always check the tensor shapes to account for broadcasting\n",
    "dbngains = (bnraw * dpre_activation_1).sum(0, keepdim=True)\n",
    "cmp(\"bngains\", dbngains, bngains)\n",
    "\n",
    "dbnraw = bngains * dpre_activation_1\n",
    "cmp(\"bnraw\", dbnraw, bnraw)\n",
    "\n",
    "dbnoffsets = dpre_activation_1.sum(0, keepdim=True)\n",
    "cmp(\"bnoffsets\", dbnoffsets, bnoffsets)\n",
    "\n",
    "# Partial\n",
    "dbndiff = bnvar_inverse * dbnraw\n",
    "cmp(\"bndiff\", dbndiff, bndiff)\n",
    "\n",
    "dbnvar_inverse = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "cmp(\"dbnvar_inverse\", dbnvar_inverse, bnvar_inverse)\n",
    "\n",
    "dbnvar = dbnvar_inverse * (-0.5) * (bnvar + EPSILON)**(-1.5)\n",
    "cmp(\"bnvar\", dbnvar, bnvar)\n",
    "\n",
    "# Also account for bessel's correction\n",
    "dbndiff_squared =  ((1/(BATCH_SIZE-1) * dbnvar)).sum(0, keepdim=True)\n",
    "cmp(\"bndiff_squared\", dbndiff_squared, bndiff_squared)\n",
    "\n",
    "dbndiff += 2 * bndiff * dbndiff_squared\n",
    "cmp(\"bndiff\", dbndiff, bndiff)\n",
    "\n",
    "# Partial\n",
    "dpre_batchnorm = dbndiff.clone()\n",
    "cmp(\"pre_batchnorm\", dpre_batchnorm, pre_batchnorm)\n",
    "\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
    "cmp(\"bnmeani\", dbnmeani, bnmeani)\n",
    "\n",
    "dpre_batchnorm += ((1/BATCH_SIZE) * dbnmeani).sum(0, keepdim=True)\n",
    "cmp(\"pre_batchnorm\", dpre_batchnorm, pre_batchnorm)\n",
    "\n",
    "dWORD_EMBEDDINGS_CAT = dpre_batchnorm @ W1.T\n",
    "cmp(\"WORD_EMBEDDINGS_CAT\", dWORD_EMBEDDINGS_CAT, WORD_EMBEDDINGS_CAT)\n",
    "\n",
    "dW1 = WORD_EMBEDDINGS_CAT.T @ dpre_batchnorm\n",
    "cmp(\"W1\", dW1, W1)\n",
    "\n",
    "db1 = dpre_batchnorm.sum(0, keepdim=True)\n",
    "cmp(\"b1\", db1, b1)\n",
    "\n",
    "dWORD_EMBEDDINGS = dWORD_EMBEDDINGS_CAT.view(WORD_EMBEDDINGS.shape)\n",
    "dCHAR_EMBEDDINGS = torch.zeros_like(CHAR_EMBEDDINGS)\n",
    "for i in range(inputs[batch_idx].shape[0]):\n",
    "    for j in range(inputs[batch_idx].shape[1]):\n",
    "        idx = inputs[batch_idx][i,j]\n",
    "        dCHAR_EMBEDDINGS[idx] += dWORD_EMBEDDINGS[i,j]\n",
    "cmp(\"CHAR_EMBEDDINGS\", dCHAR_EMBEDDINGS, CHAR_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "fefb03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Simplified NLL Loss\n",
    "\n",
    "# Before\n",
    "# Cross entropy\n",
    "# logit_maxes = logits.max(1, keepdim=True).values # Find max of each output tensor\n",
    "# norm_logits = logits - logit_maxes # Subtract corresp. max for math convenience\n",
    "# counts = norm_logits.exp() # Exponentiate\n",
    "# counts_sum = counts.sum(1, keepdim=True) # Count sum of corres. output tensor to normalize \n",
    "#                                          # and prepare to convert counts to prob\n",
    "# counts_sum_inverse = counts_sum**-1\n",
    "# probs = counts * counts_sum_inverse # Convert to probabilities\n",
    "# logprobs = probs.log() # Take log\n",
    "# loss = -logprobs[range(BATCH_SIZE), batch_labels].mean() # Compute NLL Loss for current batch\n",
    "\n",
    "# Simplified\n",
    "\n",
    "loss_simplified = F.cross_entropy(logits, batch_labels)\n",
    "print(loss_simplified - loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "4bfc5289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits               | exact: False | approximate: True | maxdiff: 6.51925802230835e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-317-4785259f559f>:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  dlogits = F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa6eb416460>"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now corresponding simplified backward prop for loss\n",
    "# if correct index -> P[i] - 1 or P[i]\n",
    "\n",
    "dlogits = F.softmax(logits)\n",
    "dlogits[range(BATCH_SIZE), batch_labels] -= 1\n",
    "# Average of losses \n",
    "dlogits /= BATCH_SIZE\n",
    "\n",
    "cmp(\"logits\", dlogits, logits)\n",
    "\n",
    "# dlogits is a zero-sum game\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "4fc6e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# BatchNorm simplified\n",
    "\n",
    "# Before:\n",
    "# # Batch Norm\n",
    "# # Mean = Sum / no. of elements; Variance\n",
    "# bnmeani = pre_batchnorm.sum(0, keepdim=True) * (1/BATCH_SIZE)\n",
    "# bndiff = pre_batchnorm - bnmeani\n",
    "# bndiff_squared = bndiff**2\n",
    "# bnvar = bndiff_squared.sum(0, keepdim=True) * (1/(BATCH_SIZE-1)) # Bessel's correction\n",
    "# bnvar_inverse = (bnvar + EPSILON)**-0.5\n",
    "# # Normalize \n",
    "# bnraw = bndiff * bnvar_inverse\n",
    "# # Preactivation\n",
    "# pre_activation_1 = bngains * bnraw + bnoffsets \n",
    "\n",
    "# After:\n",
    "pre_activation_1_new = bngains * ((pre_batchnorm - pre_batchnorm.mean(0, keepdim=True)) / torch.sqrt(pre_batchnorm.var(0, keepdim=True, unbiased=True) + EPSILON)) + bnoffsets\n",
    "print((pre_activation_1_new - pre_activation_1).abs().max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "b30b8a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_batchnorm        | exact: True | approximate: True | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "dprebatch_norm = bngains * bnvar_inverse / BATCH_SIZE * (BATCH_SIZE * dpre_batchnorm - dpre_batchnorm.sum(0) - BATCH_SIZE/(BATCH_SIZE-1) * bnraw * (dpre_batchnorm * bnraw).sum(0))\n",
    "\n",
    "cmp('pre_batchnorm', dpre_batchnorm, pre_batchnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1dc8ff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 4.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-320-26e63d307841>:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  dlogits = F.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10000/ 200000: 2.3959\n",
      "  20000/ 200000: 2.4026\n",
      "  30000/ 200000: 2.2622\n",
      "  40000/ 200000: 2.4245\n",
      "  50000/ 200000: 2.5859\n",
      "  60000/ 200000: 2.6045\n",
      "  70000/ 200000: 2.2434\n",
      "  80000/ 200000: 2.5718\n",
      "  90000/ 200000: 2.7896\n",
      " 100000/ 200000: 2.6688\n",
      " 110000/ 200000: 2.3515\n",
      " 120000/ 200000: 2.0737\n",
      " 130000/ 200000: 2.5030\n",
      " 140000/ 200000: 2.5210\n",
      " 150000/ 200000: 2.6199\n",
      " 160000/ 200000: 2.3820\n",
      " 170000/ 200000: 2.4522\n",
      " 180000/ 200000: 2.1904\n",
      " 190000/ 200000: 2.3545\n"
     ]
    }
   ],
   "source": [
    "# Putting them all together ..\n",
    "EMBEDDING_DIMENSION = 10\n",
    "VOCABULARY_SIZE = 27 # 26 alphabets and one special start end char -> \".\"\n",
    "HIDDEN_LAYER_NEURONS = 100\n",
    "\n",
    "CHAR_EMBEDDINGS = torch.randn((VOCABULARY_SIZE, EMBEDDING_DIMENSION)) \n",
    "WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs]\n",
    "\n",
    "W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS)) * ((5/3) / (EMBEDDING_DIMENSION * BLOCK_SIZE)**0.5)\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS) * 0.1\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE)) * ((5/3) / HIDDEN_LAYER_NEURONS**0.5)\n",
    "b2 = torch.randn(VOCABULARY_SIZE) * 0.1\n",
    "\n",
    "# Mul by 0.1 as 1s and 0s mask out smaller changes in grad computation. But we just \n",
    "# want to see those as well during this experiment\n",
    "\n",
    "bngains = torch.randn((1, HIDDEN_LAYER_NEURONS)) * 0.1 + 1.0\n",
    "bnoffsets = torch.randn((1, HIDDEN_LAYER_NEURONS)) * 0.1\n",
    "\n",
    "bnmean_running = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "bnstd_running = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2, bngains, bnoffsets]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "    \n",
    "# Expand all functions as much as possible to become BackProp Ninja !\n",
    "losses = []\n",
    "\n",
    "STEP_SIZE = 200000\n",
    "BATCH_SIZE = 32\n",
    "EPSILON = 1e-5\n",
    "\n",
    "# Step size vs Epoch\n",
    "with torch.no_grad():\n",
    "    for step in range(STEP_SIZE):\n",
    "        batch_idx = torch.randint(0, inputs.shape[0], (BATCH_SIZE,))\n",
    "\n",
    "        # Prepping input to the network\n",
    "        WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs[batch_idx]]\n",
    "        WORD_EMBEDDINGS_CAT = WORD_EMBEDDINGS.view(WORD_EMBEDDINGS.shape[0], -1)\n",
    "        batch_labels = labels[batch_idx]\n",
    "\n",
    "        # Layer 1 \n",
    "        pre_batchnorm = WORD_EMBEDDINGS_CAT @ W1 + b1\n",
    "        # Batch Norm\n",
    "        pre_activation_1 = bngains * ((pre_batchnorm - pre_batchnorm.mean(0, keepdim=True)) / torch.sqrt(pre_batchnorm.var(0, keepdim=True, unbiased=True) + EPSILON)) + bnoffsets\n",
    "        # Activation\n",
    "        h = torch.tanh(pre_activation_1)\n",
    "\n",
    "        # Layer 2\n",
    "        logits = h @ W2 + b2\n",
    "\n",
    "        # Cross entropy\n",
    "        loss = F.cross_entropy(logits, batch_labels)\n",
    "\n",
    "        # Zero grad\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        \n",
    "        dlogits = F.softmax(logits)\n",
    "        dlogits[range(BATCH_SIZE), batch_labels] -= 1\n",
    "        dlogits /= BATCH_SIZE\n",
    "        \n",
    "        dh = dlogits @ W2.T\n",
    "        db2 = dlogits.sum(0)\n",
    "        dW2 = h.T @ dlogits\n",
    "        \n",
    "        dpre_activation_1 = (1 - h**2) * dh\n",
    "        dbngains = (bnraw * dpre_activation_1).sum(0, keepdim=True)\n",
    "        dbnoffsets = dpre_activation_1.sum(0, keepdim=True)\n",
    "        dprebatch_norm = bngains * bnvar_inverse / BATCH_SIZE * (BATCH_SIZE * dpre_batchnorm - dpre_batchnorm.sum(0) - BATCH_SIZE/(BATCH_SIZE-1) * bnraw * (dpre_batchnorm * bnraw).sum(0))\n",
    "        dWORD_EMBEDDINGS_CAT = dpre_batchnorm @ W1.T\n",
    "        dW1 = WORD_EMBEDDINGS_CAT.T @ dpre_batchnorm\n",
    "        db1 = dpre_batchnorm.sum(0)\n",
    "        dWORD_EMBEDDINGS = dWORD_EMBEDDINGS_CAT.view(WORD_EMBEDDINGS.shape)\n",
    "        dCHAR_EMBEDDINGS = torch.zeros_like(CHAR_EMBEDDINGS)\n",
    "        for i in range(inputs[batch_idx].shape[0]):\n",
    "            for j in range(inputs[batch_idx].shape[1]):\n",
    "                idx = inputs[batch_idx][i,j]\n",
    "                dCHAR_EMBEDDINGS[idx] += dWORD_EMBEDDINGS[i,j]\n",
    "        \n",
    "        grads = [dCHAR_EMBEDDINGS, dW1, dW2, db1, db2, dbngains, dbnoffsets]\n",
    "\n",
    "        \n",
    "        lr = 0.1 if step < 100000 else 0.01\n",
    "    \n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data += -lr * grad\n",
    "        \n",
    "        if step % 10000 == 0: \n",
    "            print(f'{step:7d}/{STEP_SIZE:7d}: {loss.item():.4f}')\n",
    "            losses.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f92d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
