{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b0005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_content = \"\"\n",
    "with open(\"../code/input.txt\") as f:\n",
    "    input_content = f.read()\n",
    "    \n",
    "chars = sorted(list(set(input_content)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = torch.tensor(encode(input_content), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffad0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == \"train\" else val\n",
    "    idx = torch.randint((len(data) - BLOCK_SIZE), (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in idx])\n",
    "    return x, y\n",
    "    \n",
    "batchx, batchy = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0ff111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " phz!OPyqc&yqMOQJ?QRXyqVoZAqfwnLp\n",
      "Tn\n",
      "lJRRl .g,KYW&kA:n3m'lu\n",
      " GtNx .qc!gmH: KYx,'VyyQRsALZA3\n",
      "Cp?n,3aV\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(VOCAB_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embedding(idx)   # B,T,C\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)   # But torch needs B,C,T \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # Take last T from B,T,C\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "        \n",
    "bgm = BigramLM()\n",
    "# check forward pass\n",
    "out, loss = bgm(batchx, batchy)\n",
    "# check generation\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4355ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3QJuzje AUNQv?GFSgJlzr3-fYaSg\n",
      "CSNHWbJlfGK:'eGSWrCgD-JritjwU&os;.s\n",
      "rpoatIRBgZqqetmu,pKq3pn.lSvdvwXd:uEsXovSgnf&YW-q\n",
      "CSNU:LFs vWbaL3NR $A&y;wdMZHfSZlDqc\n",
      "dqzgorGsJWvpKG:&HWsA:FcmfAUgfaWVyxkMOTb'l?x3$FtSyRD-j;iRv3&GAj\n",
      "rbpc.qLE,Uqu'fXCiY:ki TGayqe;.oBWfSZKDsTMl\n",
      "TX.HC3r\n",
      "uAb oMYRTQvdwJkJLB.z&cb ,Ty&kCMOPyqN.Du,ua!BpjcgCC3a-tH:a.OOA.qsK IZ,Zbl\n",
      "\n",
      "TueQehNgnTiasCwJkbi XTyqONmTAkUAIdhl&ynfgft !IjhBzQI,z3AGF.;wpqKzFlbSgPz3vHfNSLvWzN,CSg:JVYWaNC:&YORL&TAOQJWzq agcjl.$vJCSyBEpQjS3uOtB'oxIFQeMY,ovnwurH!qct?ETFYQ\n"
     ]
    }
   ],
   "source": [
    "# Simple Bigram, only taking T-1 for predicting T\n",
    "\n",
    "optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)\n",
    "# experiment optimization\n",
    "batch_size = 32\n",
    "for _ in range(10000):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = bgm(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf9b5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Attention experiment with BoW  \n",
    "# Lets try to average the last N chars, not ideal, but lets try\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Approach 1\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True) # (T,T) @ (B,T,C) ==> (B,T,T) @ (B,T,C) = (B, T, C)\n",
    "result_1 = wei @ x\n",
    "\n",
    "# Approach 2\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# Affinities wont be constant -> each present will be influenced diff by diff past words\n",
    "wei = torch.zeros(T, T)\n",
    "# future cant communicate with the present\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "result_2 = wei @ x\n",
    "torch.allclose(result_1, result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb8d5d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "jVHIlYYR3S3FUGuIL\n",
      "tPofYpw3DILxkWbGJpnfyuLTou&.fDb.mT?WCmpaHdDnbjMSSEbJ'pbGXiPbaUQLAEwDIGHoa3yTDGG3nULANf'akIbabksFuXs3cL NGeBEE\n",
      "zX,3fIaew.UaxTxnoffxoqLnb-dvT Lwo:JgTmLMg-EEsWPtWCG33;DnIncA dOtlajGTpa'bf:YF\n",
      "xqL\n",
      "YYaMjq!SrRfdxGb3fDZRZMy-F abnmxYGuHx3,\n",
      "m!xmLPbGzAU J3.cEIfhRE-GLBoyYrV?3 kuLPbBpXt:bbGACwTaYJzu'af;suDoHHIafaHRd3;klWE wRbyHfbMb3oyyb;IabHQrnnj3IbbUef3EcOakHJ33MmXiqT'G.,.-naXKbG'jIR?rLsvR:MLNnQenHMYB,GJCG  NP3;,Uyy3mL-uaf3IVCLcAAURLVPVPffBXjS3yUXejb-qVQL?,;uaUHLfm ,mruFmiuPp?xU JPBW &QxAa\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 32\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(EMBED_SIZE, head_size)\n",
    "        self.query = torch.nn.Linear(EMBED_SIZE, head_size)\n",
    "        self.value = torch.nn.Linear(EMBED_SIZE, head_size)\n",
    "        self.tril = torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        wei = query @ key.transpose(-2,-1) * C**-0.5 # B,T,16 @ B, 16, T -> B, T,T -> T,T like square dist matrix\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, -float(\"inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        value = self.value(x)\n",
    "        out = wei @ value\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class BigramLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "        self.pos_embedding = torch.nn.Embedding(BLOCK_SIZE, EMBED_SIZE)\n",
    "        # insert self attention head\n",
    "        self.self_att_head = Head(EMBED_SIZE)\n",
    "        self.lm_head = torch.nn.Linear(EMBED_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)   # B,T,C\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)   # But torch needs B,C,T \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "bgm = BigramLM()\n",
    "# check forward pass\n",
    "out, loss = bgm(batchx, batchy)\n",
    "# check generation\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93906753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2859, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "\n",
      "LO:\n",
      "BRI he.\n",
      "I'ccuthes s;\n",
      "The man jun.\n",
      "\n",
      "fiud y:\n",
      "Omveravin.\n",
      "SAs adl hme wit w areaney uy ormowind thornin'lod\n",
      "Wheit mer, wifistr pe s CHankilthers I:\n",
      "Ankon peemyous gevererethea wise'nkfAny, thre R' in;,,\n",
      "D kicorard,\n",
      "\n",
      "Acesor the MAnals, h bu, l Iem aralat rd mokndevomfuthouch qmade my\n",
      "\n",
      "UERDpede t s\n",
      "Bor t t m se\n",
      "\n",
      "Whe,\n",
      "\n",
      "Teavided e, crt tBEYothardifoo, quromend\n",
      "\n",
      "Ange if teed;\n",
      "\n",
      "An.\n",
      "TangheMI be re, f mands theans wig gin,\n",
      "Bavet lld phened del-dtr n. mbechenthanthld\n",
      "LI: mer,\n",
      "Thay tsooor bbli be I ou, w\n"
     ]
    }
   ],
   "source": [
    "# Lets train new BM with Self attention head\n",
    "\n",
    "# Simple Bigram, only taking T-1 for predicting T\n",
    "bgm = BigramLM()\n",
    "optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)\n",
    "# experiment optimization\n",
    "batch_size = 32\n",
    "for _ in range(10000):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = bgm(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9bc37559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "class MultiAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, no_of_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([Head(head_size) for _ in range(no_of_heads)])\n",
    "        self.proj = torch.nn.Linear(head_size * no_of_heads, EMBED_SIZE)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f46ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update BigramLM\n",
    "NO_OF_HEADS = 4\n",
    "\n",
    "class BigramLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "        self.pos_embedding = torch.nn.Embedding(BLOCK_SIZE, EMBED_SIZE)\n",
    "        # insert self attention head\n",
    "        self.heads = MultiAttentionHead(NO_OF_HEADS, EMBED_SIZE//NO_OF_HEADS)\n",
    "        self.lm_head = torch.nn.Linear(EMBED_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)   # B,T,C\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.heads(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)   # But torch needs B,C,T \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train new BM with Self attention head\n",
    "bgm = BigramLM()\n",
    "optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "losses = []\n",
    "for _ in range(10000):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = bgm(x,y)\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(np.mean(losses))\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=500)[0].tolist()))\n",
    "# Loss : 2.6064284039855004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "29732966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding few more components\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, EMBED_SIZE):\n",
    "        super().__init__()\n",
    "        # * by 4 acc to paper.\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(EMBED_SIZE, 4 * EMBED_SIZE),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Linear(4 * EMBED_SIZE, EMBED_SIZE)) # Projection\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, no_of_heads):\n",
    "        super().__init__()\n",
    "        head_size = EMBED_SIZE // no_of_heads\n",
    "        self.sa = MultiAttentionHead(no_of_heads, head_size)\n",
    "        self.ffd = FeedForward(EMBED_SIZE)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(EMBED_SIZE)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(EMBED_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connections\n",
    "        # Pre-norm formulation\n",
    "        x = x + self.sa(self.layer_norm_1(x))\n",
    "        x = x + self.ffd(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "757c362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update BigramLM\n",
    "NO_OF_HEADS = 4\n",
    "NO_OF_BLOCKS = 4\n",
    "\n",
    "class BigramLM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "        self.pos_embedding = torch.nn.Embedding(BLOCK_SIZE, EMBED_SIZE)\n",
    "        # insert blocks\n",
    "        self.blocks = torch.nn.Sequential(*[Block(EMBED_SIZE, 4) for _ in range(NO_OF_BLOCKS)])\n",
    "        self.lm_head = torch.nn.Linear(EMBED_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)   # B,T,C\n",
    "        pos_emb = self.pos_embedding(torch.arange(T))\n",
    "        x = tok_emb + pos_emb \n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)   # But torch needs B,C,T \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "93c01978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2917728568553923\n",
      "\n",
      "He dow.\n",
      "\n",
      "AREN ROVICOLH:\n",
      "Bursing--lucher undun aved lauke murkemss.\n",
      "\n",
      "MICIOVWAxh; our me whast mure a shall fore wardy;\n",
      "Will thitth, O, how, our that poon not\n",
      "And withest heave.\n",
      "Wurtame fingume,\n",
      "Hard is sarlond\n",
      "Lut infid withou how to i men vill hard you welconder thange is\n",
      "Bucone thy thhe bu lovertumed:\n",
      "To but not bacther bon will hand nopsice\n",
      "souquy iperothe tho.\n",
      "\n",
      "AgELICIIFLA:\n",
      "I my vost that he verein.\n",
      "\n",
      "CAPurmund wito hat ard dupresty Pelloi macork's nour thou a lat hath andand have howsif love,\n"
     ]
    }
   ],
   "source": [
    "# Lets train new BM with Self attention head\n",
    "bgm = BigramLM()\n",
    "optimizer = torch.optim.AdamW(bgm.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "losses = []\n",
    "for _ in range(10000):\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits, loss = bgm(x,y)\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(np.mean(losses))\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(bgm.generate(idx, max_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea37115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
