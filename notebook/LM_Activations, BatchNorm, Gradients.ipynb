{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2afc579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292999dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names =[]\n",
    "with open(\"../code/names.txt\") as file:\n",
    "    names = file.read().split(\"\\n\")\n",
    "    \n",
    "stoi = {c:i for i, c in enumerate(string.ascii_lowercase)}\n",
    "itos = {c:i for i, c in stoi.items()}\n",
    "\n",
    "stoi[\".\"] = 26\n",
    "itos[26] = \".\"\n",
    "\n",
    "BLOCK_SIZE = 3\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    context = [26] * BLOCK_SIZE\n",
    "    for c in name + \".\":\n",
    "        label = stoi[c]\n",
    "        inputs.append(context)\n",
    "        labels.append(label)\n",
    "#         print(\"\".join(itos[i] for i in context), '-->', itos[label])\n",
    "        context = context[1:] + [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c1f0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f3b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 10\n",
    "VOCABULARY_SIZE = 27 # 26 alphabets and one special start end char -> \".\"\n",
    "HIDDEN_LAYER_NEURONS = 100\n",
    "\n",
    "# each character is a nD vector, randomly initialize char embedding\n",
    "CHAR_EMBEDDINGS = torch.randn((VOCABULARY_SIZE, EMBEDDING_DIMENSION)) \n",
    "WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs]\n",
    "\n",
    "# rand initialize weights and biases for Layer 1 \n",
    "W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS))\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS)\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE))\n",
    "b2 = torch.randn(VOCABULARY_SIZE)\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2acceea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1628432273864746\n"
     ]
    }
   ],
   "source": [
    "# Use Dynamic Learning Rates\n",
    "\n",
    "losses = []\n",
    "\n",
    "# We can use mini-batches too\n",
    "\n",
    "EPOCHS = 200000\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Require grad\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    batch_idx = torch.randint(0, inputs.shape[0], (BATCH_SIZE,))\n",
    "    \n",
    "    # Forward pass\n",
    "    WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs[batch_idx]]\n",
    "    h = torch.tanh(WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    \n",
    "    # Loss\n",
    "    loss = F.cross_entropy(logits, labels[batch_idx])\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = 0.1 if epoch < 100000 else 0.01 # step learning rate decay\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def481a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lece.',\n",
       " 'jamo.',\n",
       " 'vine.',\n",
       " 'siah.',\n",
       " 'bribn.',\n",
       " 'cassia.',\n",
       " 'aley.',\n",
       " 'killia.',\n",
       " 'mia.',\n",
       " 'thindad.',\n",
       " 'myleen.',\n",
       " 'ellawon.',\n",
       " 'darlyn.',\n",
       " 'ayai.',\n",
       " 'salhomieh.',\n",
       " 'coarin.',\n",
       " 'ediana.',\n",
       " 'jivrieton.',\n",
       " 'cain.',\n",
       " 'tria.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infer from model\n",
    "\n",
    "g = torch.Generator().manual_seed(124578798232)\n",
    "idx = 0\n",
    "BLOCK_SIZE = 3\n",
    "NO_OF_WORDS = 20\n",
    "words = []\n",
    "\n",
    "for i in range(NO_OF_WORDS):\n",
    "    word = \"\"\n",
    "    context = [26] * BLOCK_SIZE\n",
    "    \n",
    "    while True:\n",
    "        WORD_EMBEDDINGS = CHAR_EMBEDDINGS[torch.tensor([context])]\n",
    "        h = torch.tanh(WORD_EMBEDDINGS.view(1,-1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        pred_idx = torch.multinomial(logits, num_samples=1, replacement=True, generator=g).item()\n",
    "        context = context[1:] + [pred_idx]\n",
    "        word += itos[pred_idx]\n",
    "        \n",
    "        if pred_idx == 26:\n",
    "            words.append(word)\n",
    "            break\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c768625e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss\n",
    "-torch.log(torch.tensor(1/VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1213fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS)) * 0.1\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS) * 0\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE)) * 0.1\n",
    "b2 = torch.randn(VOCABULARY_SIZE) * 0\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f0cabff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Many h values go to extremes, -1 +1 due to tanh\n",
    "h = torch.tanh(WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1)\n",
    "plt.hist(h.tolist(), 50);\n",
    "plt.figure()\n",
    "plt.hist((WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd781d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights by gain, manually\n",
    "\n",
    "W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS)) * ((5/3) / (EMBEDDING_DIMENSION * BLOCK_SIZE)**0.5)\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS) * 0.01\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE)) * ((5/3) / HIDDEN_LAYER_NEURONS**0.5)\n",
    "b2 = torch.randn(VOCABULARY_SIZE) * 0.01\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2]\n",
    "# Many h values go to extremes, -1 +1 due to tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65b2e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do BatchNorm, instead\n",
    "pre_activation = WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1\n",
    "\n",
    "# make it gaussian, subtract mean and divide by STD\n",
    "pre_activation = pre_activation - pre_activation.mean(0, keepdim=True) / pre_activation.std(0, keepdim=True)\n",
    "h = torch.tanh(pre_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05baf79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm with gains and shift offsets\n",
    "bngains = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "bnoffsets = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "pre_activation = WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1\n",
    "\n",
    "pre_activation = bngains * \\ \n",
    "    (pre_activation - pre_activation.mean(0, keepdim=True) / pre_activation.std(0, keepdim=True)) \\\n",
    "    + bnoffsets\n",
    "h = torch.tanh(pre_activation)\n",
    "\n",
    "parameters = [CHAR_EMBEDDINGS, W1, W2, b1, b2, bngains, bnoffsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f946e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Mean, Std over entire training set\n",
    "\n",
    "pre_activation = WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1 + b1\n",
    "\n",
    "with torch.no_grad():\n",
    "    bnmean_all = pre_activation.mean(0, keepdim=True) \n",
    "    bnstd_all = pre_activation.std(0, keepdim=True)\n",
    "\n",
    "pre_activation = bngains * (pre_activation - bnmean_all / bnstd_all) + bnoffsets\n",
    "h = torch.tanh(pre_activation)\n",
    "\n",
    "# Use them in mini-batches until next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89592324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running bnmean_all and bnstd_all\n",
    "bnmean_running = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "bnstd_running = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "bnmeani = pre_activation.mean(0, keepdim=True)\n",
    "bnstdi = pre_activation.std(0, keepdim=True)\n",
    "   \n",
    "pre_activation = bngains * (pre_activation - bnmean_running) / bnstd_running + bnoffsets\n",
    "\n",
    "with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0c81124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.134913444519043\n"
     ]
    }
   ],
   "source": [
    "# We can ignore bias if we are BatchNorming after the h calculation.\n",
    "\n",
    "\n",
    "# W1 = torch.randn((EMBEDDING_DIMENSION * BLOCK_SIZE, HIDDEN_LAYER_NEURONS)) * ((5/3) / (EMBEDDING_DIMENSION * BLOCK_SIZE)**0.5)\n",
    "# # b1 = torch.randn(HIDDEN_LAYER_NEURONS) * 0.01\n",
    "# W2 = torch.randn((HIDDEN_LAYER_NEURONS, VOCABULARY_SIZE)) * ((5/3) / HIDDEN_LAYER_NEURONS**0.5)\n",
    "# b2 = torch.randn(VOCABULARY_SIZE) * 0.01\n",
    "\n",
    "# bngains = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "# bnoffsets = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "\n",
    "# bnmean_running = torch.zeros((1, HIDDEN_LAYER_NEURONS))\n",
    "# bnstd_running = torch.ones((1, HIDDEN_LAYER_NEURONS))\n",
    "\n",
    "# parameters = [CHAR_EMBEDDINGS, W1, W2, b2, bngains, bnoffsets]\n",
    "\n",
    "# losses = []\n",
    "\n",
    "STEP_SIZE = 200000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Step size vs Epoch\n",
    "\n",
    "for step in range(STEP_SIZE):\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    batch_idx = torch.randint(0, inputs.shape[0], (BATCH_SIZE,))\n",
    "\n",
    "    WORD_EMBEDDINGS = CHAR_EMBEDDINGS[inputs[batch_idx]]\n",
    "    pre_activation = WORD_EMBEDDINGS.view(-1,EMBEDDING_DIMENSION * BLOCK_SIZE) @ W1\n",
    "    \n",
    "    bnmeani = pre_activation.mean(0, keepdim=True)\n",
    "    bnstdi = pre_activation.std(0, keepdim=True)\n",
    "    \n",
    "    pre_activation = bngains * (pre_activation - bnmean_running) / bnstd_running + bnoffsets\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    \n",
    "    h = torch.tanh(pre_activation) #+ b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, labels[batch_idx])\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1 if step < 100000 else 0.01 # step learning rate decay\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7699a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
